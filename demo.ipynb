{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521a82da58f67d3b",
   "metadata": {},
   "source": [
    "# Evaluation of Topic Modelling\n",
    "\n",
    "Please install the relevant packages for the evaluation: \n",
    "- sentence_transformers\n",
    "- umap\n",
    "- hdbscan\n",
    "- sklearn\n",
    "- bertopic\n",
    "- datasets\n",
    "- octis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:45:36.893382Z",
     "start_time": "2023-09-24T22:45:36.888498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shu8981/anaconda3/envs/deeplearning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/shu8981/anaconda3/envs/deeplearning/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/shu8981/anaconda3/envs/deeplearning/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/shu8981/anaconda3/envs/deeplearning/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/shu8981/anaconda3/envs/deeplearning/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from umap.umap_ import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176e74106f2e8f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:45:58.815265Z",
     "start_time": "2023-09-24T22:45:58.375053Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,\n",
    "                        gen_min_span_tree=True,\n",
    "                        prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2))\n",
    "model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=5,\n",
    "    min_topic_size=10,\n",
    "    language='english',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938f927e4d687333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:46:52.289511Z",
     "start_time": "2023-09-24T22:46:46.341399Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Batches: 100%|████████████████████████████████| 153/153 [00:12<00:00, 12.15it/s]\n",
      "2023-09-25 00:28:58,529 - BERTopic - Transformed documents to Embeddings\n",
      "2023-09-25 00:29:06,721 - BERTopic - Reduced dimensionality\n",
      "2023-09-25 00:29:06,813 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Loading a sample test set from the Hugging Face's dataset repository\n",
    "from datasets import load_dataset\n",
    "hf_dataset = 'HHousen/ParaSCI'\n",
    "dataset = load_dataset(hf_dataset, split=\"test\")\n",
    "\n",
    "# Extracting the 'sentence1' field from each data entry to create a list of documents/sentences\n",
    "docs = [data['sentence1'] for data in list(dataset)]\n",
    "\n",
    "# Applying the BERTopic model on the sample list of documents\n",
    "# This will return two lists:\n",
    "# - 'topics': a list where each entry is the topic assigned to the corresponding document\n",
    "# - 'probs': a list of probabilities associated with each topic assignment\n",
    "topics, probs = model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8084e76f7f81a61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:59:42.677920Z",
     "start_time": "2023-09-24T22:59:42.366598Z"
    }
   },
   "outputs": [],
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "\n",
    "# Retrieving the vectorizer model from BERTopic and building an analyzer \n",
    "# The analyzer will be a function to split the documents into tokens (words or n-grams)\n",
    "vectorizer = model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "tokens = [analyzer(doc) for doc in docs]\n",
    "\n",
    "def get_metrics(topk=5):\n",
    "    \"\"\"Prepare evaluation measures using OCTIS\"\"\"\n",
    "    \n",
    "    # Initializing Coherence measure with the \"c_npmi\" (normalized pointwise mutual information) metric\n",
    "    # This metric evaluates the semantic similarity between the top words in each topic\n",
    "    # This metric represents Topic Coherence\n",
    "    npmi = Coherence(texts=tokens, topk=topk, measure=\"c_npmi\")\n",
    "    \n",
    "    # Initializing TopicDiversity measure\n",
    "    # This metric evaluates how diverse the top words are across different topics\n",
    "    topic_diversity = TopicDiversity(topk=topk)\n",
    "\n",
    "    # Grouping the metrics into categories for clarity\n",
    "    coherence = [(npmi, \"npmi\")]\n",
    "    diversity = [(topic_diversity, \"diversity\")]\n",
    "\n",
    "    # Combining the metrics into a list\n",
    "    metrics = [(coherence, \"Coherence\"), (diversity, \"Diversity\")]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146286e3411c3a09",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T22:59:42.681922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the function defined in the previous block to get evaluation metrics\n",
    "metrics = get_metrics()\n",
    "\n",
    "# Retrieving the top 10 words for each topic identified by BERTopic\n",
    "# Each topic is represented as a list of words\n",
    "bertopic_topics = [\n",
    "    [\n",
    "        vals[0]  # Get the word from the tuple (word, weight)\n",
    "        for vals in model.get_topic(i)[:10]  # Retrieve top 10 words for topic 'i'\n",
    "    ]\n",
    "    for i in range(len(set(topics)) - 1)  # Loop through all unique topics\n",
    "]\n",
    "\n",
    "# Organizing the retrieved topics into a dictionary format for further processing or evaluation\n",
    "output_tm = {\"topics\": bertopic_topics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6156cdcf03628bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:59:42.685749Z",
     "start_time": "2023-09-24T22:59:42.685013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.11711191329782415\n",
      "diversity: 0.4666666666666667\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Initializing an empty dictionary to store the evaluation results\n",
    "results = {}\n",
    "\n",
    "# Looping through the metrics to compute the scores for the extracted topics\n",
    "for scorers, _ in metrics:\n",
    "    for scorer, name in scorers:\n",
    "        # Using the scorer to evaluate the topics and storing the score\n",
    "        score = scorer.score(output_tm)\n",
    "        results[name] = float(score)  # Converting the score to float for consistent formatting\n",
    "\n",
    "# Printing the evaluation results, which will be used as Topic Coherence and Topic Diversity respectively\n",
    "print(\"Results\")\n",
    "print(\"============\")\n",
    "for metric, score in results.items():\n",
    "    print(f\"{metric}: {str(score)}\")\n",
    "print(\" \")  # Adding a space for cleaner output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7828915d6e96c7",
   "metadata": {},
   "source": [
    "# Evaluation of Paraphrasing\n",
    "\n",
    "Please install the relevant packages for the evaluation: \n",
    "- datasets\n",
    "- transformers\n",
    "- rouge\n",
    "- sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74349df7bfe5ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:59:42.689555Z",
     "start_time": "2023-09-24T22:59:42.687245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Loading a sample test set from the Hugging Face's dataset repository for the evaluation of paraphrasing\n",
    "from datasets import load_dataset\n",
    "hf_dataset = 'HHousen/ParaSCI'\n",
    "dataset = load_dataset(hf_dataset, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71eca34543bdb11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:59:42.690464Z",
     "start_time": "2023-09-24T22:59:42.690376Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge import Rouge \n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "# Initializing the ROUGE and BLEU metrics for evaluation\n",
    "rouge = Rouge()\n",
    "bleu = BLEU()\n",
    "\n",
    "def paraphrase(sent, en_lan, lan_en):\n",
    "    \"\"\"Paraphrase a sentence by translating it to another language and then back to the original language.\"\"\"\n",
    "    \n",
    "    # Forward translation: English to the target language (e.g., French, German, etc.)\n",
    "    \n",
    "    # Initializing the tokenizer and model for forward translation\n",
    "    forward_tokenizer = AutoTokenizer.from_pretrained(en_lan)\n",
    "    forward_model = AutoModelForSeq2SeqLM.from_pretrained(en_lan)\n",
    "    \n",
    "    # Encoding the sentence to input IDs and generating the translation\n",
    "    input_ids = forward_tokenizer.encode(sent, return_tensors=\"pt\")\n",
    "    forward_outputs = forward_model.generate(input_ids)\n",
    "    \n",
    "    # Decoding the output IDs to get the translated sentence\n",
    "    forward_decoded = forward_tokenizer.decode(forward_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Backward translation: Target language back to English\n",
    "    \n",
    "    # Initializing the tokenizer and model for backward translation\n",
    "    back_tokenizer = AutoTokenizer.from_pretrained(lan_en)\n",
    "    back_model = AutoModelForSeq2SeqLM.from_pretrained(lan_en)\n",
    "    \n",
    "    # Encoding the translated sentence to input IDs and generating the paraphrased version\n",
    "    back_input_ids = back_tokenizer.encode(forward_decoded, return_tensors=\"pt\")\n",
    "    back_outputs = back_model.generate(back_input_ids)\n",
    "    \n",
    "    # Decoding the output IDs to get the paraphrased sentence\n",
    "    back_decoded = back_tokenizer.decode(back_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return back_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7cec4bdfca42d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T22:59:42.715123Z",
     "start_time": "2023-09-24T22:59:42.699289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prefix commonly used for the Helsinki-NLP models in the Hugging Face model repository\n",
    "prefix = \"Helsinki-NLP/opus-mt-\"\n",
    "\n",
    "# Setting up model paths for different language pairs\n",
    "\n",
    "# English-German and reverse\n",
    "f1 = 'facebook/wmt19-en-de'  # Facebook's WMT English-German model\n",
    "f2 = 'Helsinki-NLP/opus-mt-gem-en'  # Helsinki-NLP's model for English-German\n",
    "\n",
    "# English-Chinese (Mandarin) and reverse\n",
    "zh1 = prefix + 'zh-en'  # Chinese to English\n",
    "zh2 = prefix + 'en-zh'  # English to Chinese\n",
    "\n",
    "# English-German and reverse\n",
    "ge1 = prefix + 'de-en'  # German to English\n",
    "ge2 = prefix + 'en-de'  # English to German\n",
    "\n",
    "# English-French and reverse\n",
    "fr1 = prefix + 'fr-en'  # French to English\n",
    "fr2 = prefix + 'en-fr'  # English to French\n",
    "\n",
    "# English-Russian and reverse\n",
    "ru1 = prefix + 'ru-en'  # Russian to English\n",
    "ru2 = prefix + 'en-ru'  # English to Russian\n",
    "\n",
    "# English-Arabic and reverse\n",
    "ar1 = prefix + 'ar-en'  # Arabic to English\n",
    "ar2 = prefix + 'en-ar'  # English to Arabic\n",
    "\n",
    "# English-Japanese and reverse\n",
    "ja1 = prefix + 'jap-en'  # Japanese to English\n",
    "ja2 = prefix + 'en-jap'  # English to Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d1b8c9cdb9711b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T22:59:42.692037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store reference paraphrases and generated paraphrases\n",
    "refs, paras = [], []\n",
    "\n",
    "# Looping through a subset of the dataset to generate paraphrases\n",
    "for data in list(dataset):\n",
    "    # Extract the sentence that needs to be paraphrased\n",
    "    to_para = data['sentence1']\n",
    "    \n",
    "    # Default setting: Generate the paraphrase by translating to Chinese (zh) and then translating back to English (en)\n",
    "    paraed = paraphrase(to_para, zh1, zh2)\n",
    "    \n",
    "    # Extract the reference paraphrase from the dataset\n",
    "    ref = data['sentence2']\n",
    "    \n",
    "    # Append the reference and generated paraphrase to the lists\n",
    "    refs.append(ref)\n",
    "    paras.append(paraed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c4e91d75705a9f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T22:59:42.694292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU Score: 0.34391269612522707\n"
     ]
    }
   ],
   "source": [
    "# Compute the corpus-level BLEU score for the generated paraphrases against the reference paraphrases\n",
    "bleu_score = bleu.corpus_score(paras, [refs])\n",
    "\n",
    "# Display the BLEU score\n",
    "print(f\"Corpus BLEU Score: {bleu_score.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f01e333d14731d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T22:59:42.697134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores: {'rouge-1': {'r': 0.00200677409268527, 'p': 0.006522342934275908, 'f': 0.00293086131506436}, 'rouge-2': {'r': 0.00030902965492835157, 'p': 0.0009451539776787983, 'f': 0.0004490302780307705}, 'rouge-l': {'r': 0.0019690569260281243, 'p': 0.0064420697134223845, 'f': 0.0028797348814809014}}\n"
     ]
    }
   ],
   "source": [
    "# Compute the average ROUGE scores for the generated paraphrases against the reference paraphrases\n",
    "rouge_scores = rouge.get_scores(paras, refs, avg=True)\n",
    "\n",
    "# Display the ROUGE scores\n",
    "print(f\"Average ROUGE Scores: {rouge_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e9782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
